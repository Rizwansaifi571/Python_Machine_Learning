{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Machine Learning - Hierarchical Clustering.__\n",
    "Date : 24, March, 2024."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Hierarchical clustering is an unsupervised learning method for clustering data points. \n",
    "\n",
    "- The algorithm builds clusters by measuring the dissimilarities between data. \n",
    "\n",
    "- Unsupervised learning means that a model does not have to be trained, and we do not need a \"target\" variable. \n",
    "\n",
    "- This method can be used on any data to visualize and interpret the relationship between individual data points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__How does it work?__\n",
    "\n",
    "- We will use Agglomerative Clustering, a type of hierarchical clustering that follows a bottom up approach.\n",
    "\n",
    "- We begin by treating each data point as its own cluster. Then, we join clusters together that have the shortest distance between them to create larger clusters. This step is repeated until one large cluster is formed containing all of the data points.\n",
    "\n",
    "- Hierarchical clustering requires us to decide on both a distance and linkage method. We will use euclidean distance and the Ward linkage method, which attempts to minimize the variance between clusters.\n",
    "\n",
    "- Example : Now we compute the ward linkage using euclidean distance, and visualize it using a dendrogram:"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
